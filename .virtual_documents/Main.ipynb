
import sys
# CPU-only build (fastest to get started, works everywhere)
!{sys.executable} -m pip install torch --index-url https://download.pytorch.org/whl/cpu



import sys
!{sys.executable} -m pip install yfinance



import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import root_mean_squared_error


device = torch.device('cpu')


ticker = 'AAPL'
df = yf.download(ticker, '2020-01-01')
display(df.head(10))


df.Close.plot(figsize = (12, 8))


scaler = StandardScaler()
# scale the closing data for our model to a normal distribution around 1
df['Close'] = scaler.fit_transform(df['Close']) 


df['Close'].head()


#clarify the data for our neural network
#give a data range so our model can predict the closing price for the next day
seq_length = 31
data = []
for i in range(len(df) - seq_length): 
    # evaluate along the model in 30-day blocks
    #attempting to predict the value of the 31st day
    data.append(df.Close[i:i + seq_length])
data = np.array(data)



train_size = int(0.8*len(data))

X_train = torch.from_numpy(data[:train_size, :-1 :]).type(torch.Tensor).to(device)
Y_train = torch.from_numpy(data[:train_size, -1]).type(torch.Tensor).to(device)
X_test = torch.from_numpy(data[train_size:, :-1 :]).type(torch.Tensor).to(device)
Y_test = torch.from_numpy(data[train_size:, -1]).type(torch.Tensor).to(device)


class PredictionModel(nn.Module): #creating our model for predicition

    def __init__ (self, input_dim, hidden_dim, num_layers, output_dim):
        super(PredictionModel, self).__init__()

        self.num_layers = num_layers #number of stacked lstm layers
        self.hidden_dim = hidden_dim #hidden size of each lstm layer

        
         # Define the LSTM:
        # - input_dim: number of features per time step
        # - hidden_dim: hidden units in the LSTM
        # - num_layers: how many LSTM layers stacked
        # - batch_first=True: expects input shape (batch, seq_len, input_dim

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first= True)
        #fully connected layer mapping from hidden dimensions to the output
        self.fc = nn.Linear(hidden_dim, output_dim) 

    def forward(self, x): #forward passes through the model

        #initial version of hidden state and cell state
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device = device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim, device = device)


        #hn and cn are the final hidden and cell states with shapes
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        out = self.fc(out[:, -1, :])

        return out



        
    


model = PredictionModel(input_dim = 1, hidden_dim = 32, num_layers = 2, output_dim = 1).to(device)


criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr = 0.01)


num_epochs = 250

for i in range(num_epochs):
    Y_train_pred = model(X_train)

    
    loss = criterion(Y_train_pred, Y_train) #measure performance

    if i % 25 == 0:
        print(i, loss.item()) #print loss to see model progress

    optimizer.zero_grad()#clear old gradient
    loss.backward() #back propagate to compute new gradient 
    optimizer.step() #update parameters using the gradient


model.eval()
Y_test_pred = model(X_test)

Y_train_pred = scaler.inverse_transform(Y_train_pred.detach().cpu().numpy())
Y_train = scaler.inverse_transform(Y_train.detach().cpu().numpy())
Y_test_pred = scaler.inverse_transform(Y_test_pred.detach().cpu().numpy())
Y_test = scaler.inverse_transform(Y_test.detach().cpu().numpy())





